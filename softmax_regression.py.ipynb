{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "251bbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function for findind gradient of w and b\n",
    "def grad(X, y, w, b,a):\n",
    "    z = np.dot(X.T,w)+b\n",
    "    yh = softmax(z)\n",
    "    reg = a*w/X.shape[1]\n",
    "    gradw = np.dot(X,(yh-y))/X.shape[1] + reg\n",
    "    gradb = (-np.sum((yh-y),axis = 0))/X.shape[1]\n",
    "    return gradw, gradb\n",
    "\n",
    "#def FMSE(X, y,w,b):\n",
    " #   a = np.square(np.dot(X.T, w) + b - y)\n",
    "  #  FMSEW = (np.mean(a)/(2))\n",
    "   # return FMSEW\n",
    "    \n",
    "    \n",
    "#Soft max Function\n",
    "def softmax(z):\n",
    "    Z = np.exp(z)\n",
    "    SZ = np.sum(Z,axis=1, keepdims = True)\n",
    "    smx = Z/SZ\n",
    "    return smx\n",
    "\n",
    "#Cross entropy Function\n",
    "def cross(X,y,w,b,a):\n",
    "    z = np.dot(X.transpose(),w)+b\n",
    "    yh = softmax(z)\n",
    "    cr = -(np.sum((y*np.log(yh))))/X.shape[1]\n",
    "    return cr,yh\n",
    "\n",
    "#Calculate the accuracy of the model\n",
    "def accuracy(yh,yte):\n",
    "    Y = np.argmax(yh,axis = 1)\n",
    "    Yte = np.argmax(yte, axis = 1)\n",
    "    acc = np.mean(Y == Yte)\n",
    "    return acc\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def SGD (x, y,w, b, e, lr, a , mini):\n",
    "    for i in range(e):\n",
    "        batches = int((len(x.transpose())/mini))\n",
    "        start = 0\n",
    "        end = mini\n",
    "        for j in range(batches):    \n",
    "            xminib = x[:,start:end]\n",
    "            yminib = y[start:end,:]\n",
    "            #print(\"sixe xd:\",xminib.shape)\n",
    "            \n",
    "            ####fmseMiniB = np.mean(np.dot(xminib.transpose(),w)+b - yminib)\n",
    "            ####with respect to w\n",
    "            ###fmseMiniB = fmseMiniBe(xminib,yminib,w,b)\n",
    "            ###fmseMiniW = fmseMiniWe(xminib,yminib,w,b,a)\n",
    "            \n",
    "            gradw,gradb = grad(xminib, yminib, w,b,a)\n",
    "            \n",
    "            wnew = w - (np.dot(lr, gradw))\n",
    "            bnew = b - (np.dot(lr, gradb))\n",
    "            #print(\"hfihjihfih\")\n",
    "            start = end\n",
    "            end = end+mini\n",
    "        \n",
    "        #calculating the cross entropy after every batch\n",
    "        costf,_ = cross(x,y,wnew, bnew,a)\n",
    "        costf += (a/2)*(np.sum(np.dot(wnew.transpose(),wnew)))\n",
    "        #print(\"cross entropy:\",costf)\n",
    "        #fmse = np.mean(np.square(np.dot(x.transpose(),wnew)+bnew-y))\n",
    "    return [costf, wnew, bnew]          \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def MNIST ():\n",
    "    # Load data\n",
    "    X_tr = np.reshape(np.load(\"fashion_mnist_train_images.npy\"), (-1, 28*28))\n",
    "    ytr = np.load(\"fashion_mnist_train_labels.npy\")\n",
    "    X_te = np.reshape(np.load(\"fashion_mnist_test_images.npy\"), (-1, 28*28))\n",
    "    yte = np.load(\"fashion_mnist_test_labels.npy\")\n",
    "    yT = yte\n",
    "    #X_tr = X_tr/255\n",
    "    \n",
    "    #X_te = X_te/255\n",
    "   \n",
    "\n",
    "    #divivding the data set 80 - 20\n",
    "    X = X_tr.transpose()\n",
    "    index_values = np.random.permutation(X.shape[1])\n",
    "\n",
    "    #print(X.shape)\n",
    "    indx = np.random.permutation(X.shape[1])\n",
    "    XTrain = X[:, index_values[:int(X.shape[1] * 0.8)]]\n",
    "    XValid = X[:, index_values[int(X.shape[1] * 0.8):]]\n",
    "    ytrain = ytr[index_values[:int(X.shape[1] * 0.8)]]\n",
    "    yTrain = np.zeros((ytrain.size, ytrain.max() + 1))\n",
    "    yTrain[np.arange(ytrain.size), ytrain] = 1\n",
    "    yvalid = ytr[index_values[int(X.shape[1] * 0.8):]]\n",
    "    yValid = np.zeros((yvalid.size, yvalid.max() + 1))\n",
    "    yValid[np.arange(yvalid.size), yvalid] = 1\n",
    "    \n",
    "    X_te = X_te.transpose()\n",
    "    yte = np.zeros((yT.size, yT.max() + 1))\n",
    "    yte[np.arange(yT.size), yT] = 1\n",
    "    #yte = yte/255\n",
    "    #yTrain = yTrain/255\n",
    "    \n",
    "    #print(\"train:\",XTrain.shape)\n",
    "    #print(yTrain.shape)\n",
    "    #print(\"valid:\",XValid.shape)\n",
    "    #print(yValid.shape)\n",
    "    \n",
    "    #random initial weights\n",
    "    w = np.random.randint(0,1,int(X.shape[0]))\n",
    "    w = np.atleast_2d(w).T\n",
    "    #considering initial bias as 0\n",
    "    b = np.zeros(10)\n",
    "    #print(w.shape)\n",
    "    \n",
    "\n",
    "    e = [80,90,100,150]\n",
    "    lr = [0.000001,0.000002,0.000003,0.000005]\n",
    "    a = [0.01,0.03,0.04,0.05]\n",
    "    mini = [100, 200, 400, 600]\n",
    "    \n",
    "    #intialising the Cost function to have an extremely large value\n",
    "    fceinitial = np.random.uniform(100000000 , 1000000000)\n",
    "    for ne in e:\n",
    "        for nlr in lr:\n",
    "            for na in a:\n",
    "                for nmini in mini:\n",
    "                    cost_,w,b = SGD(XTrain,yTrain,w,b,ne,nlr,na,nmini)\n",
    "                    fceValid,_ = cross(XValid,yValid,w,b,na)\n",
    "                    if fceValid<fceinitial:\n",
    "                        print(\"epochs \", ne,\" Learning rate \",nlr,\"alpha \", na,\" mini batch\",nmini)\n",
    "                        fceinitial = fceValid\n",
    "                        print(\"Minimum fce\",fceinitial)\n",
    "                        enew = ne\n",
    "                        lrnew = nlr\n",
    "                        anew = na\n",
    "                        mininew = nmini\n",
    "    \n",
    "                  \n",
    "    wTrain, bTrain, fmseTrain = SGD(XTrain,yTrain,w,b,enew, lrnew, anew, mininew)\n",
    "    \n",
    "    #for test set \n",
    "    \n",
    "    \n",
    "    #fmseTest = np.mean(np.square(np.dot(X_te.transpose(),yte, wTrain,b)))\n",
    "    print(\"FMSE for Test:  \",fmseTest)\n",
    "    \n",
    "    fceTest, yh = cross(X_te, yte, w,b,a)\n",
    "    acc = accuracy(yh,yte)\n",
    "    print(\"fce on Validation:\", fceValid, \"On Test:\",fceTest,\"accuracy :\",acc)\n",
    "    print(\"Final epochs \", ne,\" Final Learning rate \",nlr,\"Final alpha \", na,\" Final mini batch\",nmini)\n",
    "    #w,b = linear_regression(X_tr, ytr)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs  80  Learning rate  1e-06 alpha  0.01  mini batch 100\n",
      "Minimum fce 2.162194176363755\n",
      "epochs  80  Learning rate  1e-06 alpha  0.01  mini batch 200\n",
      "Minimum fce 2.049886561221096\n",
      "epochs  80  Learning rate  1e-06 alpha  0.01  mini batch 400\n",
      "Minimum fce 1.9434862962886634\n",
      "epochs  80  Learning rate  1e-06 alpha  0.01  mini batch 600\n",
      "Minimum fce 1.8556924593543527\n",
      "epochs  80  Learning rate  1e-06 alpha  0.03  mini batch 100\n",
      "Minimum fce 1.787945134728501\n",
      "epochs  80  Learning rate  1e-06 alpha  0.03  mini batch 200\n",
      "Minimum fce 1.7228499937345048\n",
      "epochs  80  Learning rate  1e-06 alpha  0.03  mini batch 400\n",
      "Minimum fce 1.6538157438103123\n",
      "epochs  80  Learning rate  1e-06 alpha  0.03  mini batch 600\n",
      "Minimum fce 1.5983085671187451\n",
      "epochs  80  Learning rate  1e-06 alpha  0.04  mini batch 100\n",
      "Minimum fce 1.5576712823949246\n",
      "epochs  80  Learning rate  1e-06 alpha  0.04  mini batch 200\n",
      "Minimum fce 1.5163505500854244\n",
      "epochs  80  Learning rate  1e-06 alpha  0.04  mini batch 400\n",
      "Minimum fce 1.4682422446546617\n",
      "epochs  80  Learning rate  1e-06 alpha  0.04  mini batch 600\n",
      "Minimum fce 1.4306259343875305\n",
      "epochs  80  Learning rate  1e-06 alpha  0.05  mini batch 100\n",
      "Minimum fce 1.4044006143109664\n",
      "epochs  80  Learning rate  1e-06 alpha  0.05  mini batch 200\n",
      "Minimum fce 1.37666555104311\n",
      "epochs  80  Learning rate  1e-06 alpha  0.05  mini batch 400\n",
      "Minimum fce 1.3409684155938606\n",
      "epochs  80  Learning rate  1e-06 alpha  0.05  mini batch 600\n",
      "Minimum fce 1.3139021307750585\n",
      "epochs  80  Learning rate  2e-06 alpha  0.01  mini batch 100\n",
      "Minimum fce 1.2957467795844289\n",
      "epochs  80  Learning rate  2e-06 alpha  0.01  mini batch 200\n",
      "Minimum fce 1.2477086091676945\n",
      "epochs  80  Learning rate  2e-06 alpha  0.01  mini batch 400\n",
      "Minimum fce 1.1910413310111774\n",
      "epochs  80  Learning rate  2e-06 alpha  0.01  mini batch 600\n",
      "Minimum fce 1.1735414431356606\n",
      "epochs  80  Learning rate  2e-06 alpha  0.03  mini batch 100\n",
      "Minimum fce 1.1570804075730092\n",
      "epochs  80  Learning rate  2e-06 alpha  0.03  mini batch 200\n",
      "Minimum fce 1.1426945406955809\n",
      "epochs  80  Learning rate  2e-06 alpha  0.03  mini batch 400\n",
      "Minimum fce 1.089305531290123\n",
      "epochs  80  Learning rate  2e-06 alpha  0.03  mini batch 600\n",
      "Minimum fce 1.0884549670713628\n",
      "epochs  80  Learning rate  2e-06 alpha  0.04  mini batch 100\n",
      "Minimum fce 1.0706759110519408\n",
      "epochs  80  Learning rate  2e-06 alpha  0.04  mini batch 200\n",
      "Minimum fce 1.070640026240834\n",
      "epochs  80  Learning rate  2e-06 alpha  0.04  mini batch 400\n",
      "Minimum fce 1.0205761053426892\n",
      "epochs  80  Learning rate  2e-06 alpha  0.05  mini batch 100\n",
      "Minimum fce 1.0086513178306944\n",
      "epochs  80  Learning rate  2e-06 alpha  0.05  mini batch 400\n",
      "Minimum fce 0.969803318401615\n",
      "epochs  80  Learning rate  3e-06 alpha  0.01  mini batch 600\n",
      "Minimum fce 0.9573332327032725\n",
      "epochs  80  Learning rate  3e-06 alpha  0.03  mini batch 600\n",
      "Minimum fce 0.9384762311158913\n",
      "epochs  80  Learning rate  3e-06 alpha  0.04  mini batch 400\n",
      "Minimum fce 0.926946357425936\n",
      "epochs  80  Learning rate  3e-06 alpha  0.05  mini batch 400\n",
      "Minimum fce 0.858795618626121\n",
      "epochs  80  Learning rate  5e-06 alpha  0.05  mini batch 100\n",
      "Minimum fce 0.848130481556131\n",
      "epochs  80  Learning rate  5e-06 alpha  0.05  mini batch 600\n",
      "Minimum fce 0.8335988078849632\n",
      "epochs  90  Learning rate  1e-06 alpha  0.01  mini batch 100\n",
      "Minimum fce 0.768726561674605\n",
      "epochs  90  Learning rate  1e-06 alpha  0.01  mini batch 200\n",
      "Minimum fce 0.7451296450915266\n",
      "epochs  90  Learning rate  1e-06 alpha  0.01  mini batch 400\n",
      "Minimum fce 0.7418978398565205\n",
      "epochs  90  Learning rate  1e-06 alpha  0.01  mini batch 600\n",
      "Minimum fce 0.7405863982352799\n",
      "epochs  90  Learning rate  1e-06 alpha  0.03  mini batch 100\n",
      "Minimum fce 0.7350939219341409\n",
      "epochs  90  Learning rate  1e-06 alpha  0.04  mini batch 100\n",
      "Minimum fce 0.7310699893100868\n",
      "epochs  90  Learning rate  1e-06 alpha  0.05  mini batch 100\n",
      "Minimum fce 0.7284746463137599\n",
      "epochs  90  Learning rate  2e-06 alpha  0.01  mini batch 100\n",
      "Minimum fce 0.7274547573597179\n",
      "epochs  90  Learning rate  2e-06 alpha  0.01  mini batch 400\n",
      "Minimum fce 0.7269073943341634\n",
      "epochs  90  Learning rate  2e-06 alpha  0.03  mini batch 100\n",
      "Minimum fce 0.7228415680682254\n",
      "epochs  90  Learning rate  2e-06 alpha  0.03  mini batch 400\n",
      "Minimum fce 0.7220549752224393\n",
      "epochs  90  Learning rate  2e-06 alpha  0.04  mini batch 100\n",
      "Minimum fce 0.7183308247936936\n",
      "epochs  90  Learning rate  2e-06 alpha  0.04  mini batch 400\n",
      "Minimum fce 0.7177115534577533\n",
      "epochs  90  Learning rate  2e-06 alpha  0.05  mini batch 100\n",
      "Minimum fce 0.7141197424211849\n",
      "epochs  90  Learning rate  2e-06 alpha  0.05  mini batch 400\n",
      "Minimum fce 0.7136892607781699\n"
     ]
    }
   ],
   "source": [
    "MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65a51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6633b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
